{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aef5b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07bb8a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a21b800d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/anaconda3/lib/python3.11/site-packages/pyspark'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52d8522f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/25 05:04:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n",
      "|_c0|_c1| _c2|\n",
      "+---+---+----+\n",
      "|111|zzz|8000|\n",
      "|111|aaa|8888|\n",
      "|121|bbb|8000|\n",
      "+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "empdf=spark.read.csv(\"file:///home/labuser/datasets/emp.csv\")\n",
    "\n",
    "empdf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cd0f1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-9-21.ap-south-1.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Spark SQL basic example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f138c3d1010>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2e13569",
   "metadata": {},
   "outputs": [],
   "source": [
    "salesdf = spark.read.csv(\"/home/labuser/datasets/superstore.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf74a159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string, _c13: string, _c14: string, _c15: string, _c16: string, _c17: string, _c18: string, _c19: string, _c20: string, _c21: string, _c22: string, _c23: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salesdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b8d26e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "51291"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salesdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9abee5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "salesdf = spark.read.option(\"header\",\"true\").csv(\"/home/labuser/datasets/superstore.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65ff73d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 7:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+----------+----------+--------------+----------+----------------+-----------+-------------+---------------+-------------+----------+------+------------+----------------+---------------+------------+--------------------+----------------+--------+--------+----------+------------+-------------+\n",
      "|   ID|        OrderID| OrderDate|  ShipDate|      ShipMode|CustomerID|    CustomerName|    Segment|         City|          State|      Country|PostalCode|Market|      Region|       ProductID|       Category|Sub-Category|         ProductName|           Sales|Quantity|Discount|    Profit|ShippingCost|OrderPriority|\n",
      "+-----+---------------+----------+----------+--------------+----------+----------------+-----------+-------------+---------------+-------------+----------+------+------------+----------------+---------------+------------+--------------------+----------------+--------+--------+----------+------------+-------------+\n",
      "|32298| CA-2012-124891|31/07/2012|31/07/2012|      Same Day|  RH-19495|     Rick Hansen|   Consumer|New York City|       New York|United States|     10024|    US|        East| TEC-AC-10003033|     Technology| Accessories|Plantronics CS510...|         2309.65|       7|       0|  762.1845|     933.57 |     Critical|\n",
      "|26341|  IN-2013-77878|05/02/2013|07/02/2013|  Second Class|  JR-16210|   Justin Ritter|  Corporate|   Wollongong|New South Wales|    Australia|      null|  APAC|     Oceania| FUR-CH-10003950|      Furniture|      Chairs|Novimex Executive...|           Black|3709.395|       9|       0.1|    -288.765|      923.63 |\n",
      "|25330|  IN-2013-71249|17/10/2013|18/10/2013|   First Class|  CR-12730|    Craig Reiter|   Consumer|     Brisbane|     Queensland|    Australia|      null|  APAC|     Oceania| TEC-PH-10004664|     Technology|      Phones|   Nokia Smart Phone|  with Caller ID|5175.171|       9|       0.1|     919.971|      915.49 |\n",
      "|13524|ES-2013-1579342|28/01/2013|30/01/2013|   First Class|  KM-16375|Katherine Murray|Home Office|       Berlin|         Berlin|      Germany|      null|    EU|     Central| TEC-PH-10004583|     Technology|      Phones|Motorola Smart Phone|        Cordless| 2892.51|       5|       0.1|      -96.54|      910.16 |\n",
      "|47221|   SG-2013-4320|05/11/2013|06/11/2013|      Same Day|   RH-9495|     Rick Hansen|   Consumer|        Dakar|          Dakar|      Senegal|      null|Africa|      Africa|TEC-SHA-10000501|     Technology|     Copiers|  Sharp Wireless Fax|      High-Speed| 2832.96|       8|         0|      311.52|      903.04 |\n",
      "|22732|  IN-2013-42360|28/06/2013|01/07/2013|  Second Class|  JM-15655|     Jim Mitchum|  Corporate|       Sydney|New South Wales|    Australia|      null|  APAC|     Oceania| TEC-PH-10000030|     Technology|      Phones| Samsung Smart Phone|  with Caller ID|2862.675|       5|       0.1|     763.275|      897.35 |\n",
      "|30570|  IN-2011-81826|07/11/2011|09/11/2011|   First Class|  TS-21340|   Toby Swindell|   Consumer|      Porirua|     Wellington|  New Zealand|      null|  APAC|     Oceania| FUR-CH-10004050|      Furniture|      Chairs|Novimex Executive...|      Adjustable| 1822.08|       4|         0|      564.84|       894.77|\n",
      "|31192|  IN-2012-86369|14/04/2012|18/04/2012|Standard Class|  MB-18085|      Mick Brown|   Consumer|     Hamilton|        Waikato|  New Zealand|      null|  APAC|     Oceania| FUR-TA-10002958|      Furniture|      Tables|Chromcraft Confer...| Fully Assembled| 5244.84|       6|         0|      996.48|       878.38|\n",
      "|40155| CA-2014-135909|14/10/2014|21/10/2014|Standard Class|  JW-15220|       Jane Waco|  Corporate|   Sacramento|     California|United States|     95823|    US|        West| OFF-BI-10003527|Office Supplies|     Binders|Fellowes PB500 El...|         5083.96|       5|     0.2|  1906.485|     867.69 |          Low|\n",
      "|40936| CA-2012-116638|28/01/2012|31/01/2012|  Second Class|  JH-15985|     Joseph Holt|   Consumer|      Concord| North Carolina|United States|     28027|    US|       South| FUR-TA-10000198|      Furniture|      Tables|Chromcraft Bull-N...|        4297.644|      13|     0.4|-1862.3124|     865.74 |     Critical|\n",
      "|34577| CA-2011-102988|05/04/2011|09/04/2011|  Second Class|  GM-14695|    Greg Maxwell|  Corporate|   Alexandria|       Virginia|United States|     22304|    US|       South| OFF-SU-10002881|Office Supplies|    Supplies|Martin Yale Chadl...|         4164.05|       5|       0|    83.281|     846.54 |         High|\n",
      "|28879|  ID-2012-28402|19/04/2012|22/04/2012|   First Class|  AJ-10780|  Anthony Jacobs|  Corporate|        Kabul|          Kabul|  Afghanistan|      null|  APAC|Central Asia| FUR-TA-10001889|      Furniture|      Tables|Bevis Conference ...| Fully Assembled| 4626.15|       5|         0|      647.55|      835.57 |\n",
      "|45794|   SA-2011-1830|27/12/2011|29/12/2011|  Second Class|   MM-7260| Magdelene Morse|   Consumer|        Jizan|          Jizan| Saudi Arabia|      null|  EMEA|        EMEA|TEC-CIS-10001717|     Technology|      Phones|   Cisco Smart Phone|  with Caller ID| 2616.96|       4|         0|      1151.4|      832.41 |\n",
      "| 4132| MX-2012-130015|13/11/2012|13/11/2012|      Same Day|  VF-21715|  Vicky Freymann|Home Office|       Toledo|         Parana|       Brazil|      null| LATAM|       South| FUR-CH-10002033|      Furniture|      Chairs|Harbour Creations...|      Adjustable|  2221.8|       7|         0|      622.02|      810.25 |\n",
      "|27704|  IN-2013-73951|06/06/2013|08/06/2013|  Second Class|  PF-19120|    Peter Fuller|   Consumer|   Mudanjiang|   Heilongjiang|        China|      null|  APAC|  North Asia| OFF-AP-10003500|Office Supplies|  Appliances|KitchenAid Microwave|           White| 3701.52|      12|         0|     1036.08|      804.54 |\n",
      "|13779|ES-2014-5099955|31/07/2014|03/08/2014|  Second Class|  BP-11185|    Ben Peterman|  Corporate|        Paris|  Ile-de-France|       France|      null|    EU|     Central| OFF-AP-10000423|Office Supplies|  Appliances|Breville Refriger...|             Red|1869.588|       4|       0.1|     186.948|      801.66 |\n",
      "|36178| CA-2014-143567|03/11/2014|06/11/2014|  Second Class|  TB-21175|   Thomas Boland|  Corporate|    Henderson|       Kentucky|United States|     42420|    US|       South| TEC-AC-10004145|     Technology| Accessories|Logitech diNovo E...|         2249.91|       9|       0|  517.4793|     780.70 |     Critical|\n",
      "|12069|ES-2014-1651774|08/09/2014|14/09/2014|Standard Class|  PJ-18835|   Patrick Jones|  Corporate|        Prato|        Tuscany|        Italy|      null|    EU|       South| OFF-AP-10004512|Office Supplies|  Appliances|        Hoover Stove|             Red| 7958.58|      14|         0|     3979.08|      778.32 |\n",
      "|22096|  IN-2014-11763|31/01/2014|01/02/2014|   First Class|  JS-15685|        Jim Sink|  Corporate|   Townsville|     Queensland|    Australia|      null|  APAC|     Oceania| TEC-CO-10000865|     Technology|     Copiers| Brother Fax Machine|      High-Speed|2565.594|       9|       0.1|      28.404|      766.93 |\n",
      "|49463|   TZ-2014-8190|05/12/2014|07/12/2014|  Second Class|   RH-9555| Ritsa Hightower|   Consumer|       Uvinza|         Kigoma|     Tanzania|      null|Africa|      Africa|OFF-KIT-10004058|Office Supplies|  Appliances|    KitchenAid Stove|           White| 3409.74|       6|         0|      818.28|      763.38 |\n",
      "+-----+---------------+----------+----------+--------------+----------+----------------+-----------+-------------+---------------+-------------+----------+------+------------+----------------+---------------+------------+--------------------+----------------+--------+--------+----------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salesdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b7a6367",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeedf = spark.read.option(\"header\",\"true\").csv(\"/home/labuser/datasets/employee.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba83e7fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+\n",
      "|empno|ename| sal|\n",
      "+-----+-----+----+\n",
      "|  111|  zzz|8000|\n",
      "|  111|  aaa|8888|\n",
      "|  121|  bbb|8000|\n",
      "| NULL|  ccc|9000|\n",
      "|false|  ddd|6000|\n",
      "|  555|  eee|7000|\n",
      "|  765| null|null|\n",
      "|  666|  fff|8890|\n",
      "|  aaa| null|null|\n",
      "| True| null|null|\n",
      "| true| null|null|\n",
      "|    a|    b|   c|\n",
      "|    x| NULL|NULL|\n",
      "| 1000| null|null|\n",
      "|  222| NULL|NULL|\n",
      "+-----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeedf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f22a0d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empno: string (nullable = true)\n",
      " |-- ename: string (nullable = true)\n",
      " |-- sal: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeedf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0dc888ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- OrderID: string (nullable = true)\n",
      " |-- OrderDate: string (nullable = true)\n",
      " |-- ShipDate: string (nullable = true)\n",
      " |-- ShipMode: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- CustomerName: string (nullable = true)\n",
      " |-- Segment: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- PostalCode: string (nullable = true)\n",
      " |-- Market: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- ProductID: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Sub-Category: string (nullable = true)\n",
      " |-- ProductName: string (nullable = true)\n",
      " |-- Sales: string (nullable = true)\n",
      " |-- Quantity: string (nullable = true)\n",
      " |-- Discount: string (nullable = true)\n",
      " |-- Profit: string (nullable = true)\n",
      " |-- ShippingCost: string (nullable = true)\n",
      " |-- OrderPriority: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salesdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc3581f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "salesdf = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"/home/labuser/datasets/superstore.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b8aaab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- OrderID: string (nullable = true)\n",
      " |-- OrderDate: string (nullable = true)\n",
      " |-- ShipDate: string (nullable = true)\n",
      " |-- ShipMode: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- CustomerName: string (nullable = true)\n",
      " |-- Segment: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- PostalCode: integer (nullable = true)\n",
      " |-- Market: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- ProductID: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Sub-Category: string (nullable = true)\n",
      " |-- ProductName: string (nullable = true)\n",
      " |-- Sales: string (nullable = true)\n",
      " |-- Quantity: string (nullable = true)\n",
      " |-- Discount: string (nullable = true)\n",
      " |-- Profit: string (nullable = true)\n",
      " |-- ShippingCost: string (nullable = true)\n",
      " |-- OrderPriority: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salesdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd4b0c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empno: string (nullable = true)\n",
      " |-- ename: string (nullable = true)\n",
      " |-- sal: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeedf = spark.read.option(\"header\",\"true\").csv(\"/home/labuser/datasets/employee.csv\", inferSchema = True)\n",
    "employeedf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64783bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user defined schema: recommended\n",
    "from pyspark.sql.types import StructType,StructField, StringType,IntegerType,DoubleType\n",
    "\n",
    "EmpSchema = StructType([  \n",
    "    StructField('Empno', IntegerType(), True), \n",
    "    StructField('Empname', StringType(), True),    \n",
    "     StructField('salary', DoubleType(), True) \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fcfb8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeedf = spark.read.option(\"header\",\"true\").schema(EmpSchema).csv(\"/home/labuser/datasets/employee.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50c8523b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Empno: integer (nullable = true)\n",
      " |-- Empname: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeedf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f270cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+\n",
      "|Empno|Empname|salary|\n",
      "+-----+-------+------+\n",
      "|  111|    zzz|8000.0|\n",
      "|  111|    aaa|8888.0|\n",
      "|  121|    bbb|8000.0|\n",
      "| null|    ccc|9000.0|\n",
      "| null|    ddd|6000.0|\n",
      "|  555|    eee|7000.0|\n",
      "|  765|   null|  null|\n",
      "|  666|    fff|8890.0|\n",
      "| null|   null|  null|\n",
      "| null|   null|  null|\n",
      "| null|   null|  null|\n",
      "| null|      b|  null|\n",
      "| null|   NULL|  null|\n",
      "| 1000|   null|  null|\n",
      "|  222|   NULL|  null|\n",
      "+-----+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/25 05:05:07 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: empno, ename, sal\n",
      " Schema: Empno, Empname, salary\n",
      "Expected: Empname but found: ename\n",
      "CSV file: file:///home/labuser/datasets/employee.csv\n"
     ]
    }
   ],
   "source": [
    "employeedf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0afc20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeedf_malformed=spark.read.option(\"header\",\"true\").option(\"mode\",\"dropmalformed\").schema(EmpSchema).csv(\"/home/labuser/datasets/employee.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c37f14c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+\n",
      "|Empno|Empname|salary|\n",
      "+-----+-------+------+\n",
      "|  111|    zzz|8000.0|\n",
      "|  111|    aaa|8888.0|\n",
      "|  121|    bbb|8000.0|\n",
      "|  555|    eee|7000.0|\n",
      "|  666|    fff|8890.0|\n",
      "+-----+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/25 05:05:07 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: empno, ename, sal\n",
      " Schema: Empno, Empname, salary\n",
      "Expected: Empname but found: ename\n",
      "CSV file: file:///home/labuser/datasets/employee.csv\n"
     ]
    }
   ],
   "source": [
    "employeedf_malformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8ac8c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeedf_failfast=spark.read.option(\"header\",\"true\").option(\"mode\",\"failfast\").schema(EmpSchema).csv(\"/home/labuser/datasets/employee.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5e31130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/25 05:05:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: empno, ename, sal\n",
      " Schema: Empno, Empname, salary\n",
      "Expected: Empname but found: ename\n",
      "CSV file: file:///home/labuser/datasets/employee.csv\n",
      "23/09/25 05:05:08 ERROR Executor: Exception in task 0.0 in stage 16.0 (TID 17)\n",
      "org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING] Malformed records are detected in record parsing: [null,ccc,9000.0].\n",
      "Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1764)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"NULL\"\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n",
      "\t... 24 more\n",
      "Caused by: java.lang.NumberFormatException: For input string: \"NULL\"\n",
      "\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n",
      "\tat java.lang.Integer.parseInt(Integer.java:580)\n",
      "\tat java.lang.Integer.parseInt(Integer.java:615)\n",
      "\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\n",
      "\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\n",
      "\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:189)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:189)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:189)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n",
      "\t... 27 more\n",
      "23/09/25 05:05:08 WARN TaskSetManager: Lost task 0.0 in stage 16.0 (TID 17) (ip-172-31-9-21.ap-south-1.compute.internal executor driver): org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING] Malformed records are detected in record parsing: [null,ccc,9000.0].\n",
      "Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1764)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"NULL\"\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n",
      "\t... 24 more\n",
      "Caused by: java.lang.NumberFormatException: For input string: \"NULL\"\n",
      "\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n",
      "\tat java.lang.Integer.parseInt(Integer.java:580)\n",
      "\tat java.lang.Integer.parseInt(Integer.java:615)\n",
      "\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\n",
      "\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\n",
      "\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:189)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:189)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:189)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n",
      "\t... 27 more\n",
      "\n",
      "23/09/25 05:05:08 ERROR TaskSetManager: Task 0 in stage 16.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o107.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 17) (ip-172-31-9-21.ap-south-1.compute.internal executor driver): org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING] Malformed records are detected in record parsing: [null,ccc,9000.0].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1764)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:69)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"NULL\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 24 more\nCaused by: java.lang.NumberFormatException: For input string: \"NULL\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Integer.parseInt(Integer.java:580)\n\tat java.lang.Integer.parseInt(Integer.java:615)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n\t... 27 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING] Malformed records are detected in record parsing: [null,ccc,9000.0].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1764)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:69)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"NULL\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 24 more\nCaused by: java.lang.NumberFormatException: For input string: \"NULL\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Integer.parseInt(Integer.java:580)\n\tat java.lang.Integer.parseInt(Integer.java:615)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n\t... 27 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m employeedf_failfast\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    894\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    895\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    896\u001b[0m     )\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 899\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;241m20\u001b[39m, vertical))\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o107.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 17) (ip-172-31-9-21.ap-south-1.compute.internal executor driver): org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING] Malformed records are detected in record parsing: [null,ccc,9000.0].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1764)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:69)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"NULL\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 24 more\nCaused by: java.lang.NumberFormatException: For input string: \"NULL\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Integer.parseInt(Integer.java:580)\n\tat java.lang.Integer.parseInt(Integer.java:615)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n\t... 27 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING] Malformed records are detected in record parsing: [null,ccc,9000.0].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1764)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:69)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"NULL\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 24 more\nCaused by: java.lang.NumberFormatException: For input string: \"NULL\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Integer.parseInt(Integer.java:580)\n\tat java.lang.Integer.parseInt(Integer.java:615)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n\t... 27 more\n"
     ]
    }
   ],
   "source": [
    "employeedf_failfast.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f19e24ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51290"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salesdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3106494d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(ID=32298, OrderID='CA-2012-124891', OrderDate='31/07/2012', ShipDate='31/07/2012', ShipMode='Same Day', CustomerID='RH-19495', CustomerName='Rick Hansen', Segment='Consumer', City='New York City', State='New York', Country='United States', PostalCode=10024, Market='US', Region='East', ProductID='TEC-AC-10003033', Category='Technology', Sub-Category='Accessories', ProductName='Plantronics CS510 - Over-the-Head monaural Wireless Headset System', Sales='2309.65', Quantity='7', Discount='0', Profit='762.1845', ShippingCost=' 933.57 ', OrderPriority='Critical')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salesdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85d3f158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salesdf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7f467bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+----------+----------+------------+----------+-------------+---------+-------------+---------------+-------------+----------+------+-------+---------------+----------+------------+--------------------+-------+--------+--------+--------+------------+-------------+\n",
      "|   ID|       OrderID| OrderDate|  ShipDate|    ShipMode|CustomerID| CustomerName|  Segment|         City|          State|      Country|PostalCode|Market| Region|      ProductID|  Category|Sub-Category|         ProductName|  Sales|Quantity|Discount|  Profit|ShippingCost|OrderPriority|\n",
      "+-----+--------------+----------+----------+------------+----------+-------------+---------+-------------+---------------+-------------+----------+------+-------+---------------+----------+------------+--------------------+-------+--------+--------+--------+------------+-------------+\n",
      "|32298|CA-2012-124891|31/07/2012|31/07/2012|    Same Day|  RH-19495|  Rick Hansen| Consumer|New York City|       New York|United States|     10024|    US|   East|TEC-AC-10003033|Technology| Accessories|Plantronics CS510...|2309.65|       7|       0|762.1845|     933.57 |     Critical|\n",
      "|26341| IN-2013-77878|05/02/2013|07/02/2013|Second Class|  JR-16210|Justin Ritter|Corporate|   Wollongong|New South Wales|    Australia|      null|  APAC|Oceania|FUR-CH-10003950| Furniture|      Chairs|Novimex Executive...|  Black|3709.395|       9|     0.1|    -288.765|      923.63 |\n",
      "+-----+--------------+----------+----------+------------+----------+-------------+---------+-------------+---------------+-------------+----------+------+-------+---------------+----------+------------+--------------------+-------+--------+--------+--------+------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salesdf.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46a9f38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(ID=32298, OrderID='CA-2012-124891', OrderDate='31/07/2012', ShipDate='31/07/2012', ShipMode='Same Day', CustomerID='RH-19495', CustomerName='Rick Hansen', Segment='Consumer', City='New York City', State='New York', Country='United States', PostalCode=10024, Market='US', Region='East', ProductID='TEC-AC-10003033', Category='Technology', Sub-Category='Accessories', ProductName='Plantronics CS510 - Over-the-Head monaural Wireless Headset System', Sales='2309.65', Quantity='7', Discount='0', Profit='762.1845', ShippingCost=' 933.57 ', OrderPriority='Critical')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salesdf.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84a52ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=32298, OrderID='CA-2012-124891', OrderDate='31/07/2012', ShipDate='31/07/2012', ShipMode='Same Day', CustomerID='RH-19495', CustomerName='Rick Hansen', Segment='Consumer', City='New York City', State='New York', Country='United States', PostalCode=10024, Market='US', Region='East', ProductID='TEC-AC-10003033', Category='Technology', Sub-Category='Accessories', ProductName='Plantronics CS510 - Over-the-Head monaural Wireless Headset System', Sales='2309.65', Quantity='7', Discount='0', Profit='762.1845', ShippingCost=' 933.57 ', OrderPriority='Critical'),\n",
       " Row(ID=26341, OrderID='IN-2013-77878', OrderDate='05/02/2013', ShipDate='07/02/2013', ShipMode='Second Class', CustomerID='JR-16210', CustomerName='Justin Ritter', Segment='Corporate', City='Wollongong', State='New South Wales', Country='Australia', PostalCode=None, Market='APAC', Region='Oceania', ProductID='FUR-CH-10003950', Category='Furniture', Sub-Category='Chairs', ProductName='Novimex Executive Leather Armchair', Sales=' Black', Quantity='3709.395', Discount='9', Profit='0.1', ShippingCost='-288.765', OrderPriority=' 923.63 ')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salesdf.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f73609a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+\n",
      "|      country|          state|\n",
      "+-------------+---------------+\n",
      "|United States|       New York|\n",
      "|    Australia|New South Wales|\n",
      "|    Australia|     Queensland|\n",
      "|      Germany|         Berlin|\n",
      "|      Senegal|          Dakar|\n",
      "|    Australia|New South Wales|\n",
      "|  New Zealand|     Wellington|\n",
      "|  New Zealand|        Waikato|\n",
      "|United States|     California|\n",
      "|United States| North Carolina|\n",
      "|United States|       Virginia|\n",
      "|  Afghanistan|          Kabul|\n",
      "| Saudi Arabia|          Jizan|\n",
      "|       Brazil|         Parana|\n",
      "|        China|   Heilongjiang|\n",
      "|       France|  Ile-de-France|\n",
      "|United States|       Kentucky|\n",
      "|        Italy|        Tuscany|\n",
      "|    Australia|     Queensland|\n",
      "|     Tanzania|         Kigoma|\n",
      "+-------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salesdf.select(\"country\", \"state\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3e4a1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1555"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salesdf.select(\"country\", \"state\").filter(\"country= 'India'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b1a7957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 28:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|country|            state|\n",
      "+-------+-----------------+\n",
      "|  India|      Maharashtra|\n",
      "|  India|    Uttar Pradesh|\n",
      "|  India|      Uttarakhand|\n",
      "|  India|           Kerala|\n",
      "|  India|     Chhattisgarh|\n",
      "|  India|       Tamil Nadu|\n",
      "|  India|            Assam|\n",
      "|  India|        Telangana|\n",
      "|  India|       Chandigarh|\n",
      "|  India|Jammu and Kashmir|\n",
      "|  India|          Manipur|\n",
      "|  India|           Odisha|\n",
      "|  India|   Andhra Pradesh|\n",
      "|  India|          Haryana|\n",
      "|  India|          Tripura|\n",
      "|  India|            Bihar|\n",
      "|  India|        Rajasthan|\n",
      "|  India|           Punjab|\n",
      "|  India|          Gujarat|\n",
      "|  India|      West Bengal|\n",
      "|  India|        Jharkhand|\n",
      "|  India|        Karnataka|\n",
      "|  India|   Madhya Pradesh|\n",
      "|  India|            Delhi|\n",
      "|  India|       Puducherry|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "salesdf.select(\"country\", \"state\").filter(\"country= 'India'\").distinct().show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94a3a394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|country|            state|\n",
      "+-------+-----------------+\n",
      "|  India|      Maharashtra|\n",
      "|  India|    Uttar Pradesh|\n",
      "|  India|      Uttarakhand|\n",
      "|  India|           Kerala|\n",
      "|  India|     Chhattisgarh|\n",
      "|  India|       Tamil Nadu|\n",
      "|  India|            Assam|\n",
      "|  India|        Telangana|\n",
      "|  India|       Chandigarh|\n",
      "|  India|Jammu and Kashmir|\n",
      "|  India|          Manipur|\n",
      "|  India|           Odisha|\n",
      "|  India|   Andhra Pradesh|\n",
      "|  India|          Haryana|\n",
      "|  India|          Tripura|\n",
      "|  India|            Bihar|\n",
      "|  India|        Rajasthan|\n",
      "|  India|           Punjab|\n",
      "|  India|          Gujarat|\n",
      "|  India|      West Bengal|\n",
      "|  India|        Jharkhand|\n",
      "|  India|        Karnataka|\n",
      "|  India|   Madhya Pradesh|\n",
      "|  India|            Delhi|\n",
      "|  India|       Puducherry|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salesdf.select(\"country\",\"state\").filter(\"country= 'India'\").dropDuplicates().show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a3e4158",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_india_df=salesdf.select(\"country\",\"state\",\"category\",\"profit\").filter(\"country='India'\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a4deea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+---------------+------+\n",
      "|country|            state|       category|profit|\n",
      "+-------+-----------------+---------------+------+\n",
      "|  India|           Kerala|Office Supplies|     0|\n",
      "|  India|          Haryana|Office Supplies|     0|\n",
      "|  India|     Chhattisgarh|Office Supplies|     0|\n",
      "|  India|   Madhya Pradesh|     Technology|     0|\n",
      "|  India|        Telangana|     Technology|     0|\n",
      "|  India|        Rajasthan|     Technology|     0|\n",
      "|  India|            Delhi|      Furniture|     0|\n",
      "|  India|Jammu and Kashmir|      Furniture|     0|\n",
      "|  India|            Assam|Office Supplies|     0|\n",
      "|  India|           Punjab|Office Supplies|     0|\n",
      "|  India|           Odisha|      Furniture|     0|\n",
      "|  India|       Tamil Nadu|Office Supplies|     0|\n",
      "|  India|      Uttarakhand|Office Supplies|     0|\n",
      "|  India|       Puducherry|Office Supplies|     0|\n",
      "|  India|          Gujarat|Office Supplies|     0|\n",
      "|  India|          Manipur|      Furniture|     0|\n",
      "|  India|          Tripura|      Furniture|     0|\n",
      "|  India|      Maharashtra|     Technology|   0.5|\n",
      "|  India|            Assam|      Furniture|     0|\n",
      "|  India|      Maharashtra|     Technology|     0|\n",
      "+-------+-----------------+---------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_india_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0a39d4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_india_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91d94455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://ip-172-31-9-21.ap-south-1.compute.internal:4040'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0cd6c400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales_india_df.write.format(\"json\").save(\"/home/labuser/sparkout/sales_india\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95e8a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# salesdf.write.format(\"parquet\").save(\"/home/labuser/sparkout/sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf31ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f7e1e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28047e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "972187ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salesdf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2c5d472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "salesdf_repart= salesdf.repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a1358f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salesdf_repart.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7905fb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/home/labuser/output/sales_repart already exists. Set mode as \"overwrite\" to overwrite the existing path.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m salesdf_repart\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/labuser/output/sales_repart\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1656\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1655\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mparquet(path)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/home/labuser/output/sales_repart already exists. Set mode as \"overwrite\" to overwrite the existing path."
     ]
    }
   ],
   "source": [
    "salesdf_repart.write.parquet(\"/home/labuser/output/sales_repart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f2012012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/home/labuser/output/sales_repart8 already exists. Set mode as \"overwrite\" to overwrite the existing path.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m salesdf\u001b[38;5;241m.\u001b[39mrepartition(\u001b[38;5;241m8\u001b[39m)\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39morc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/labuser/output/sales_repart8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1862\u001b[0m, in \u001b[0;36mDataFrameWriter.orc\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1861\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1862\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39morc(path)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/home/labuser/output/sales_repart8 already exists. Set mode as \"overwrite\" to overwrite the existing path."
     ]
    }
   ],
   "source": [
    "salesdf.repartition(8).write.orc(\"/home/labuser/output/sales_repart8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5cf653b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales_parquet=spark.read.format(\"parquet\").load(\"/home/labuser/output/sales_repart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4edf0190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51290"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_parquet.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7439b800",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_parquet1=spark.read.format(\"parquet\").load(\"/home/labuser/output/sales_repart/part-00000-1f9f992b-a39d-4d70-9d68-da8420da61aa-c000.snappy.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "36c40fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10259"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_parquet1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "029c9b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_parquet.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3a04aaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_orc=spark.read.format(\"orc\").load(\"/home/labuser/output/sales_repart8/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "054b7fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_orc.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dfd02ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_coal_df=sales_orc.coalesce(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aa034e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_coal_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "24e84eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51290"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_orc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c6c4be4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_rep=sales_orc.repartition(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "286fae10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 57:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_rep.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9ea3fc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+----------+----------+--------------+----------+------------------+-----------+----------------+--------------------+--------------------+----------+------+--------------+----------------+---------------+------------+--------------------+--------------------+--------+--------+--------+------------+-------------+\n",
      "|   ID|        OrderID| OrderDate|  ShipDate|      ShipMode|CustomerID|      CustomerName|    Segment|            City|               State|             Country|PostalCode|Market|        Region|       ProductID|       Category|Sub-Category|         ProductName|               Sales|Quantity|Discount|  Profit|ShippingCost|OrderPriority|\n",
      "+-----+---------------+----------+----------+--------------+----------+------------------+-----------+----------------+--------------------+--------------------+----------+------+--------------+----------------+---------------+------------+--------------------+--------------------+--------+--------+--------+------------+-------------+\n",
      "|32732| CA-2014-106852|21/06/2014|28/06/2014|Standard Class|  ST-20530|          Shui Tom|   Consumer|           Parma|                Ohio|       United States|     44134|    US|          East| OFF-PA-10001639|Office Supplies|       Paper|           Xerox 203|              31.104|       6|     0.2| 10.8864|       4.16 |          Low|\n",
      "|21940|  IN-2013-59888|16/10/2013|18/10/2013|   First Class|  CR-12625|       Corey Roper|Home Office|        Canberra|Australian Capita...|           Australia|      null|  APAC|       Oceania| TEC-PH-10000169|     Technology|      Phones|   Apple Smart Phone|      with Caller ID| 766.872|       2|     0.4|     114.972|      250.36 |\n",
      "|11645|ES-2011-4699764|14/03/2011|17/03/2011|  Second Class|  EB-14110|    Eugene Barchas|   Consumer|         Leipzig|              Saxony|             Germany|      null|    EU|       Central| OFF-AP-10004512|Office Supplies|  Appliances|        Hoover Stove|                 Red|3069.738|       6|     0.1|    1364.238|      725.34 |\n",
      "|43682|   SA-2014-3480|30/09/2014|05/10/2014|Standard Class|   CL-1890|       Carl Ludwig|   Consumer|          Riyadh|            Ar Riyad|        Saudi Arabia|      null|  EMEA|          EMEA|OFF-TEN-10004210|Office Supplies|     Storage|         Tenex Trays|          Wire Frame|   53.28|       1|       0|       26.64|        4.31 |\n",
      "| 9554| MX-2014-137141|13/03/2014|15/03/2014|   First Class|  SJ-20125|     Sanjit Jacobs|Home Office|        Irapuato|          Guanajuato|              Mexico|      null| LATAM|         North| OFF-AR-10003913|Office Supplies|         Art|Binney & Smith Hi...|                Blue|   44.24|       4|       0|        7.92|        8.07 |\n",
      "|35525| CA-2013-113803|25/03/2013|27/03/2013|   First Class|  VG-21805|       Vivek Grady|  Corporate|        Richmond|             Indiana|       United States|     47374|    US|       Central| OFF-PA-10001994|Office Supplies|       Paper|Ink Jet Note and ...| 8-1/2\" x 5-1/2\" ...|   22.48|       1|       0|     10.3408|        4.36 |\n",
      "|30889|  IN-2013-86586|13/05/2013|20/05/2013|Standard Class|  EA-14035|     Erin Ashbrook|  Corporate|       Whangarei|           Northland|         New Zealand|      null|  APAC|       Oceania| TEC-CO-10003924|     Technology|     Copiers|         Brother Ink|             Digital|  146.94|       1|       0|          72|        14.91|\n",
      "|24914|  IN-2013-12729|03/05/2013|07/05/2013|  Second Class|  KT-16480|     Kean Thornton|   Consumer|       Mangalore|           Karnataka|               India|      null|  APAC|  Central Asia| FUR-CH-10004609|      Furniture|      Chairs|Harbour Creations...|          Adjustable|  200.76|       2|       0|       86.28|       31.12 |\n",
      "|20805|  IN-2013-20744|26/01/2013|26/01/2013|      Same Day|  AS-10135|      Adrian Shami|Home Office|Ho Chi Minh City|    Ho Chí Minh City|             Vietnam|      null|  APAC|Southeast Asia| OFF-AR-10004486|Office Supplies|         Art|      Sanford Canvas|                Blue| 209.658|       5|    0.17|      22.608|       40.29 |\n",
      "|10433|ES-2011-1749151|13/06/2011|18/06/2011|  Second Class|  LC-16870|     Lena Cacioppo|   Consumer| Vitry-sur-Seine|       Ile-de-France|              France|      null|    EU|       Central| OFF-BI-10000972|Office Supplies|     Binders|   Acco 3-Hole Punch|             Economy|  271.08|       9|       0|        51.3|       21.03 |\n",
      "|49955|   PL-2014-4440|07/07/2014|07/07/2014|      Same Day|   PA-9060|    Pete Armstrong|Home Office|          Warsaw|             Masovia|              Poland|      null|  EMEA|          EMEA|OFF-ROG-10001340|Office Supplies|     Storage|    Rogers File Cart|          Industrial|   141.6|       1|       0|       24.06|        6.54 |\n",
      "|  532| MX-2013-149916|19/12/2013|24/12/2013|Standard Class|  MC-17605|      Matt Connell|  Corporate|   Villa Canales|           Guatemala|           Guatemala|      null| LATAM|       Central| TEC-AC-10002013|     Technology| Accessories|SanDisk Numeric K...|           Bluetooth|  481.52|      13|       0|      100.88|       88.20 |\n",
      "|43332|   RO-2014-2360|08/08/2014|08/08/2014|      Same Day|   LR-7035|         Lisa Ryan|  Corporate|       Timisoara|               Timis|             Romania|      null|  EMEA|          EMEA|OFF-TEN-10000025|Office Supplies|     Storage|       Tenex Lockers|                Blue|  204.15|       1|       0|       53.07|       26.93 |\n",
      "|43269|   TU-2013-3850|15/11/2013|22/11/2013|Standard Class|   LS-7230|  Lycoris Saunders|   Consumer|           Bursa|               Bursa|              Turkey|      null|  EMEA|          EMEA|OFF-BOS-10001511|Office Supplies|         Art|       Boston Canvas|         Fluorescent|   87.84|       4|     0.6|      -92.28|       15.62 |\n",
      "|38670| US-2014-123862|08/01/2014|10/01/2014|  Second Class|  JF-15190|      Jamie Frazer|   Consumer|      Long Beach|          California|       United States|     90805|    US|          West| OFF-SU-10004884|Office Supplies|    Supplies|Acme Galleria Hot...|              110.11|       7|       0| 31.9319|       8.84 |         High|\n",
      "|49642|   CG-2012-7880|01/08/2012|07/08/2012|Standard Class|   CW-1905|        Carl Weiss|Home Office|      Lubumbashi|             Katanga|Democratic Republ...|      null|Africa|        Africa|TEC-SHA-10003670|     Technology|     Copiers|  Sharp Wireless Fax|               Laser|  355.92|       1|       0|       88.98|       29.46 |\n",
      "|34115| CA-2012-104626|01/09/2012|08/09/2012|Standard Class|  DR-12940|     Daniel Raglin|Home Office|        Franklin|       Massachusetts|       United States|      2038|    US|          East| OFF-ST-10003208|Office Supplies|     Storage|Adjustable Depth ...|             1088.76|       6|       0|315.7404|      54.34 |       Medium|\n",
      "|49164|   MO-2011-5440|25/05/2011|30/05/2011|  Second Class|   Co-2640|        Corey-Lock|   Consumer|          Agadir|    Souss-Massa-Draâ|             Morocco|      null|Africa|        Africa|OFF-FEL-10001865|Office Supplies|     Storage|  Fellowes File Cart|          Wire Frame|  272.76|       2|       0|       57.24|       27.61 |\n",
      "|18026|ES-2014-3995946|13/08/2014|13/08/2014|      Same Day|  FH-14275|      Frank Hawley|  Corporate|       Doncaster|             England|      United Kingdom|      null|    EU|         North| TEC-CO-10000556|     Technology|     Copiers|     HP Wireless Fax|               Laser|  720.42|       2|       0|        43.2|       75.07 |\n",
      "|26172|  IN-2011-67392|05/12/2011|10/12/2011|Standard Class|  ZD-21925|Zuschuss Donatelli|   Consumer|           Dhaka|               Dhaka|          Bangladesh|      null|  APAC|  Central Asia| OFF-AR-10001405|Office Supplies|         Art|Binney & Smith Hi...|         Fluorescent|   107.1|       6|       0|       13.86|       14.80 |\n",
      "+-----+---------------+----------+----------+--------------+----------+------------------+-----------+----------------+--------------------+--------------------+----------+------+--------------+----------------+---------------+------------+--------------------+--------------------+--------+--------+--------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales_coal_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b7cc5730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 59:>                                                         (0 + 2) / 2]\r",
      "\r",
      "[Stage 59:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+----------+----------+--------------+----------+------------------+-----------+----------------+--------------------+--------------------+----------+------+--------------+----------------+---------------+------------+--------------------+--------------------+--------+--------+--------+------------+-------------+\n",
      "|   ID|        OrderID| OrderDate|  ShipDate|      ShipMode|CustomerID|      CustomerName|    Segment|            City|               State|             Country|PostalCode|Market|        Region|       ProductID|       Category|Sub-Category|         ProductName|               Sales|Quantity|Discount|  Profit|ShippingCost|OrderPriority|\n",
      "+-----+---------------+----------+----------+--------------+----------+------------------+-----------+----------------+--------------------+--------------------+----------+------+--------------+----------------+---------------+------------+--------------------+--------------------+--------+--------+--------+------------+-------------+\n",
      "|32732| CA-2014-106852|21/06/2014|28/06/2014|Standard Class|  ST-20530|          Shui Tom|   Consumer|           Parma|                Ohio|       United States|     44134|    US|          East| OFF-PA-10001639|Office Supplies|       Paper|           Xerox 203|              31.104|       6|     0.2| 10.8864|       4.16 |          Low|\n",
      "|21940|  IN-2013-59888|16/10/2013|18/10/2013|   First Class|  CR-12625|       Corey Roper|Home Office|        Canberra|Australian Capita...|           Australia|      null|  APAC|       Oceania| TEC-PH-10000169|     Technology|      Phones|   Apple Smart Phone|      with Caller ID| 766.872|       2|     0.4|     114.972|      250.36 |\n",
      "|11645|ES-2011-4699764|14/03/2011|17/03/2011|  Second Class|  EB-14110|    Eugene Barchas|   Consumer|         Leipzig|              Saxony|             Germany|      null|    EU|       Central| OFF-AP-10004512|Office Supplies|  Appliances|        Hoover Stove|                 Red|3069.738|       6|     0.1|    1364.238|      725.34 |\n",
      "|43682|   SA-2014-3480|30/09/2014|05/10/2014|Standard Class|   CL-1890|       Carl Ludwig|   Consumer|          Riyadh|            Ar Riyad|        Saudi Arabia|      null|  EMEA|          EMEA|OFF-TEN-10004210|Office Supplies|     Storage|         Tenex Trays|          Wire Frame|   53.28|       1|       0|       26.64|        4.31 |\n",
      "| 9554| MX-2014-137141|13/03/2014|15/03/2014|   First Class|  SJ-20125|     Sanjit Jacobs|Home Office|        Irapuato|          Guanajuato|              Mexico|      null| LATAM|         North| OFF-AR-10003913|Office Supplies|         Art|Binney & Smith Hi...|                Blue|   44.24|       4|       0|        7.92|        8.07 |\n",
      "|35525| CA-2013-113803|25/03/2013|27/03/2013|   First Class|  VG-21805|       Vivek Grady|  Corporate|        Richmond|             Indiana|       United States|     47374|    US|       Central| OFF-PA-10001994|Office Supplies|       Paper|Ink Jet Note and ...| 8-1/2\" x 5-1/2\" ...|   22.48|       1|       0|     10.3408|        4.36 |\n",
      "|30889|  IN-2013-86586|13/05/2013|20/05/2013|Standard Class|  EA-14035|     Erin Ashbrook|  Corporate|       Whangarei|           Northland|         New Zealand|      null|  APAC|       Oceania| TEC-CO-10003924|     Technology|     Copiers|         Brother Ink|             Digital|  146.94|       1|       0|          72|        14.91|\n",
      "|24914|  IN-2013-12729|03/05/2013|07/05/2013|  Second Class|  KT-16480|     Kean Thornton|   Consumer|       Mangalore|           Karnataka|               India|      null|  APAC|  Central Asia| FUR-CH-10004609|      Furniture|      Chairs|Harbour Creations...|          Adjustable|  200.76|       2|       0|       86.28|       31.12 |\n",
      "|20805|  IN-2013-20744|26/01/2013|26/01/2013|      Same Day|  AS-10135|      Adrian Shami|Home Office|Ho Chi Minh City|    Ho Chí Minh City|             Vietnam|      null|  APAC|Southeast Asia| OFF-AR-10004486|Office Supplies|         Art|      Sanford Canvas|                Blue| 209.658|       5|    0.17|      22.608|       40.29 |\n",
      "|10433|ES-2011-1749151|13/06/2011|18/06/2011|  Second Class|  LC-16870|     Lena Cacioppo|   Consumer| Vitry-sur-Seine|       Ile-de-France|              France|      null|    EU|       Central| OFF-BI-10000972|Office Supplies|     Binders|   Acco 3-Hole Punch|             Economy|  271.08|       9|       0|        51.3|       21.03 |\n",
      "|49955|   PL-2014-4440|07/07/2014|07/07/2014|      Same Day|   PA-9060|    Pete Armstrong|Home Office|          Warsaw|             Masovia|              Poland|      null|  EMEA|          EMEA|OFF-ROG-10001340|Office Supplies|     Storage|    Rogers File Cart|          Industrial|   141.6|       1|       0|       24.06|        6.54 |\n",
      "|  532| MX-2013-149916|19/12/2013|24/12/2013|Standard Class|  MC-17605|      Matt Connell|  Corporate|   Villa Canales|           Guatemala|           Guatemala|      null| LATAM|       Central| TEC-AC-10002013|     Technology| Accessories|SanDisk Numeric K...|           Bluetooth|  481.52|      13|       0|      100.88|       88.20 |\n",
      "|43332|   RO-2014-2360|08/08/2014|08/08/2014|      Same Day|   LR-7035|         Lisa Ryan|  Corporate|       Timisoara|               Timis|             Romania|      null|  EMEA|          EMEA|OFF-TEN-10000025|Office Supplies|     Storage|       Tenex Lockers|                Blue|  204.15|       1|       0|       53.07|       26.93 |\n",
      "|43269|   TU-2013-3850|15/11/2013|22/11/2013|Standard Class|   LS-7230|  Lycoris Saunders|   Consumer|           Bursa|               Bursa|              Turkey|      null|  EMEA|          EMEA|OFF-BOS-10001511|Office Supplies|         Art|       Boston Canvas|         Fluorescent|   87.84|       4|     0.6|      -92.28|       15.62 |\n",
      "|38670| US-2014-123862|08/01/2014|10/01/2014|  Second Class|  JF-15190|      Jamie Frazer|   Consumer|      Long Beach|          California|       United States|     90805|    US|          West| OFF-SU-10004884|Office Supplies|    Supplies|Acme Galleria Hot...|              110.11|       7|       0| 31.9319|       8.84 |         High|\n",
      "|49642|   CG-2012-7880|01/08/2012|07/08/2012|Standard Class|   CW-1905|        Carl Weiss|Home Office|      Lubumbashi|             Katanga|Democratic Republ...|      null|Africa|        Africa|TEC-SHA-10003670|     Technology|     Copiers|  Sharp Wireless Fax|               Laser|  355.92|       1|       0|       88.98|       29.46 |\n",
      "|34115| CA-2012-104626|01/09/2012|08/09/2012|Standard Class|  DR-12940|     Daniel Raglin|Home Office|        Franklin|       Massachusetts|       United States|      2038|    US|          East| OFF-ST-10003208|Office Supplies|     Storage|Adjustable Depth ...|             1088.76|       6|       0|315.7404|      54.34 |       Medium|\n",
      "|49164|   MO-2011-5440|25/05/2011|30/05/2011|  Second Class|   Co-2640|        Corey-Lock|   Consumer|          Agadir|    Souss-Massa-Draâ|             Morocco|      null|Africa|        Africa|OFF-FEL-10001865|Office Supplies|     Storage|  Fellowes File Cart|          Wire Frame|  272.76|       2|       0|       57.24|       27.61 |\n",
      "|18026|ES-2014-3995946|13/08/2014|13/08/2014|      Same Day|  FH-14275|      Frank Hawley|  Corporate|       Doncaster|             England|      United Kingdom|      null|    EU|         North| TEC-CO-10000556|     Technology|     Copiers|     HP Wireless Fax|               Laser|  720.42|       2|       0|        43.2|       75.07 |\n",
      "|26172|  IN-2011-67392|05/12/2011|10/12/2011|Standard Class|  ZD-21925|Zuschuss Donatelli|   Consumer|           Dhaka|               Dhaka|          Bangladesh|      null|  APAC|  Central Asia| OFF-AR-10001405|Office Supplies|         Art|Binney & Smith Hi...|         Fluorescent|   107.1|       6|       0|       13.86|       14.80 |\n",
      "+-----+---------------+----------+----------+--------------+----------+------------------+-----------+----------------+--------------------+--------------------+----------+------+--------------+----------------+---------------+------------+--------------------+--------------------+--------+--------+--------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales_rep.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d4ebae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "salesdf.createOrReplaceTempView(\"sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cadef6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|         |    sales|       true|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "230b86da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+------+\n",
      "|  country|            state|profit|\n",
      "+---------+-----------------+------+\n",
      "|Australia|  New South Wales|   0.1|\n",
      "|Australia|       Queensland|   0.1|\n",
      "|Australia|  New South Wales|   0.1|\n",
      "|Australia|       Queensland|   0.1|\n",
      "|Australia|       Queensland|   0.1|\n",
      "|Australia|       Queensland|   0.1|\n",
      "|Australia|Western Australia|   0.1|\n",
      "|Australia|         Victoria|   0.1|\n",
      "|Australia|  New South Wales|   0.1|\n",
      "|Australia|  New South Wales|     0|\n",
      "|Australia|  New South Wales|   0.1|\n",
      "|Australia|Western Australia|   0.1|\n",
      "|Australia|       Queensland|   0.1|\n",
      "|Australia|  New South Wales|   0.1|\n",
      "|Australia|  South Australia|   0.1|\n",
      "|Australia|  South Australia|   0.1|\n",
      "|Australia|Western Australia|   0.1|\n",
      "|Australia|       Queensland|   0.1|\n",
      "|Australia|Western Australia|   0.1|\n",
      "|Australia|  South Australia|   0.1|\n",
      "+---------+-----------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select country, state, profit from sales where country='Australia'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b8af7a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------+\n",
      "|           country|total_profit|\n",
      "+------------------+------------+\n",
      "|     United States|   265916.61|\n",
      "|            Turkey|       826.8|\n",
      "|           Nigeria|       633.5|\n",
      "|         Indonesia|      413.26|\n",
      "|         Australia|       407.2|\n",
      "|          Honduras|      290.07|\n",
      "|       Philippines|      235.55|\n",
      "|            Brazil|      232.82|\n",
      "|       Netherlands|       209.6|\n",
      "|            France|      204.35|\n",
      "|Dominican Republic|      179.98|\n",
      "|         Argentina|      168.95|\n",
      "|            Mexico|      162.93|\n",
      "|            Panama|      157.64|\n",
      "|             Italy|       137.7|\n",
      "|           Germany|       117.8|\n",
      "|       New Zealand|       117.4|\n",
      "|          Pakistan|       110.7|\n",
      "|    United Kingdom|       107.3|\n",
      "|          Thailand|      106.95|\n",
      "+------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select country, round(sum(profit),2) as total_profit from sales group by country order by total_profit desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2c1ccde5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create database shellida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2224387f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "| shellida|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "03083b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use shellida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b53d30b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# salesdf.write.saveAsTable(\"shellida.sales_perm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "35ed938b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|         |    sales|       true|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bc590750",
   "metadata": {},
   "outputs": [],
   "source": [
    "#caching\n",
    "empdf = spark.read.csv(\"/home/labuser/datasets/emp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "58ffb208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: int, OrderID: string, OrderDate: string, ShipDate: string, ShipMode: string, CustomerID: string, CustomerName: string, Segment: string, City: string, State: string, Country: string, PostalCode: int, Market: string, Region: string, ProductID: string, Category: string, Sub-Category: string, ProductName: string, Sales: string, Quantity: string, Discount: string, Profit: string, ShippingCost: string, OrderPriority: string]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salesdf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b9f9cdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "51290"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salesdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0e9a803e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n",
      "|_c0|_c1| _c2|\n",
      "+---+---+----+\n",
      "|111|zzz|8000|\n",
      "|111|aaa|8888|\n",
      "|121|bbb|8000|\n",
      "+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e0df00b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+----------+----------+--------------+----------+----------------+-----------+-------------+---------------+-------------+----------+------+------------+----------------+---------------+------------+--------------------+----------------+--------+--------+----------+------------+-------------+\n",
      "|   ID|        OrderID| OrderDate|  ShipDate|      ShipMode|CustomerID|    CustomerName|    Segment|         City|          State|      Country|PostalCode|Market|      Region|       ProductID|       Category|Sub-Category|         ProductName|           Sales|Quantity|Discount|    Profit|ShippingCost|OrderPriority|\n",
      "+-----+---------------+----------+----------+--------------+----------+----------------+-----------+-------------+---------------+-------------+----------+------+------------+----------------+---------------+------------+--------------------+----------------+--------+--------+----------+------------+-------------+\n",
      "|32298| CA-2012-124891|31/07/2012|31/07/2012|      Same Day|  RH-19495|     Rick Hansen|   Consumer|New York City|       New York|United States|     10024|    US|        East| TEC-AC-10003033|     Technology| Accessories|Plantronics CS510...|         2309.65|       7|       0|  762.1845|     933.57 |     Critical|\n",
      "|26341|  IN-2013-77878|05/02/2013|07/02/2013|  Second Class|  JR-16210|   Justin Ritter|  Corporate|   Wollongong|New South Wales|    Australia|      null|  APAC|     Oceania| FUR-CH-10003950|      Furniture|      Chairs|Novimex Executive...|           Black|3709.395|       9|       0.1|    -288.765|      923.63 |\n",
      "|25330|  IN-2013-71249|17/10/2013|18/10/2013|   First Class|  CR-12730|    Craig Reiter|   Consumer|     Brisbane|     Queensland|    Australia|      null|  APAC|     Oceania| TEC-PH-10004664|     Technology|      Phones|   Nokia Smart Phone|  with Caller ID|5175.171|       9|       0.1|     919.971|      915.49 |\n",
      "|13524|ES-2013-1579342|28/01/2013|30/01/2013|   First Class|  KM-16375|Katherine Murray|Home Office|       Berlin|         Berlin|      Germany|      null|    EU|     Central| TEC-PH-10004583|     Technology|      Phones|Motorola Smart Phone|        Cordless| 2892.51|       5|       0.1|      -96.54|      910.16 |\n",
      "|47221|   SG-2013-4320|05/11/2013|06/11/2013|      Same Day|   RH-9495|     Rick Hansen|   Consumer|        Dakar|          Dakar|      Senegal|      null|Africa|      Africa|TEC-SHA-10000501|     Technology|     Copiers|  Sharp Wireless Fax|      High-Speed| 2832.96|       8|         0|      311.52|      903.04 |\n",
      "|22732|  IN-2013-42360|28/06/2013|01/07/2013|  Second Class|  JM-15655|     Jim Mitchum|  Corporate|       Sydney|New South Wales|    Australia|      null|  APAC|     Oceania| TEC-PH-10000030|     Technology|      Phones| Samsung Smart Phone|  with Caller ID|2862.675|       5|       0.1|     763.275|      897.35 |\n",
      "|30570|  IN-2011-81826|07/11/2011|09/11/2011|   First Class|  TS-21340|   Toby Swindell|   Consumer|      Porirua|     Wellington|  New Zealand|      null|  APAC|     Oceania| FUR-CH-10004050|      Furniture|      Chairs|Novimex Executive...|      Adjustable| 1822.08|       4|         0|      564.84|       894.77|\n",
      "|31192|  IN-2012-86369|14/04/2012|18/04/2012|Standard Class|  MB-18085|      Mick Brown|   Consumer|     Hamilton|        Waikato|  New Zealand|      null|  APAC|     Oceania| FUR-TA-10002958|      Furniture|      Tables|Chromcraft Confer...| Fully Assembled| 5244.84|       6|         0|      996.48|       878.38|\n",
      "|40155| CA-2014-135909|14/10/2014|21/10/2014|Standard Class|  JW-15220|       Jane Waco|  Corporate|   Sacramento|     California|United States|     95823|    US|        West| OFF-BI-10003527|Office Supplies|     Binders|Fellowes PB500 El...|         5083.96|       5|     0.2|  1906.485|     867.69 |          Low|\n",
      "|40936| CA-2012-116638|28/01/2012|31/01/2012|  Second Class|  JH-15985|     Joseph Holt|   Consumer|      Concord| North Carolina|United States|     28027|    US|       South| FUR-TA-10000198|      Furniture|      Tables|Chromcraft Bull-N...|        4297.644|      13|     0.4|-1862.3124|     865.74 |     Critical|\n",
      "|34577| CA-2011-102988|05/04/2011|09/04/2011|  Second Class|  GM-14695|    Greg Maxwell|  Corporate|   Alexandria|       Virginia|United States|     22304|    US|       South| OFF-SU-10002881|Office Supplies|    Supplies|Martin Yale Chadl...|         4164.05|       5|       0|    83.281|     846.54 |         High|\n",
      "|28879|  ID-2012-28402|19/04/2012|22/04/2012|   First Class|  AJ-10780|  Anthony Jacobs|  Corporate|        Kabul|          Kabul|  Afghanistan|      null|  APAC|Central Asia| FUR-TA-10001889|      Furniture|      Tables|Bevis Conference ...| Fully Assembled| 4626.15|       5|         0|      647.55|      835.57 |\n",
      "|45794|   SA-2011-1830|27/12/2011|29/12/2011|  Second Class|   MM-7260| Magdelene Morse|   Consumer|        Jizan|          Jizan| Saudi Arabia|      null|  EMEA|        EMEA|TEC-CIS-10001717|     Technology|      Phones|   Cisco Smart Phone|  with Caller ID| 2616.96|       4|         0|      1151.4|      832.41 |\n",
      "| 4132| MX-2012-130015|13/11/2012|13/11/2012|      Same Day|  VF-21715|  Vicky Freymann|Home Office|       Toledo|         Parana|       Brazil|      null| LATAM|       South| FUR-CH-10002033|      Furniture|      Chairs|Harbour Creations...|      Adjustable|  2221.8|       7|         0|      622.02|      810.25 |\n",
      "|27704|  IN-2013-73951|06/06/2013|08/06/2013|  Second Class|  PF-19120|    Peter Fuller|   Consumer|   Mudanjiang|   Heilongjiang|        China|      null|  APAC|  North Asia| OFF-AP-10003500|Office Supplies|  Appliances|KitchenAid Microwave|           White| 3701.52|      12|         0|     1036.08|      804.54 |\n",
      "|13779|ES-2014-5099955|31/07/2014|03/08/2014|  Second Class|  BP-11185|    Ben Peterman|  Corporate|        Paris|  Ile-de-France|       France|      null|    EU|     Central| OFF-AP-10000423|Office Supplies|  Appliances|Breville Refriger...|             Red|1869.588|       4|       0.1|     186.948|      801.66 |\n",
      "|36178| CA-2014-143567|03/11/2014|06/11/2014|  Second Class|  TB-21175|   Thomas Boland|  Corporate|    Henderson|       Kentucky|United States|     42420|    US|       South| TEC-AC-10004145|     Technology| Accessories|Logitech diNovo E...|         2249.91|       9|       0|  517.4793|     780.70 |     Critical|\n",
      "|12069|ES-2014-1651774|08/09/2014|14/09/2014|Standard Class|  PJ-18835|   Patrick Jones|  Corporate|        Prato|        Tuscany|        Italy|      null|    EU|       South| OFF-AP-10004512|Office Supplies|  Appliances|        Hoover Stove|             Red| 7958.58|      14|         0|     3979.08|      778.32 |\n",
      "|22096|  IN-2014-11763|31/01/2014|01/02/2014|   First Class|  JS-15685|        Jim Sink|  Corporate|   Townsville|     Queensland|    Australia|      null|  APAC|     Oceania| TEC-CO-10000865|     Technology|     Copiers| Brother Fax Machine|      High-Speed|2565.594|       9|       0.1|      28.404|      766.93 |\n",
      "|49463|   TZ-2014-8190|05/12/2014|07/12/2014|  Second Class|   RH-9555| Ritsa Hightower|   Consumer|       Uvinza|         Kigoma|     Tanzania|      null|Africa|      Africa|OFF-KIT-10004058|Office Supplies|  Appliances|    KitchenAid Stove|           White| 3409.74|       6|         0|      818.28|      763.38 |\n",
      "+-----+---------------+----------+----------+--------------+----------+----------------+-----------+-------------+---------------+-------------+----------+------+------------+----------------+---------------+------------+--------------------+----------------+--------+--------+----------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salesdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5ce3735b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: int, OrderID: string, OrderDate: string, ShipDate: string, ShipMode: string, CustomerID: string, CustomerName: string, Segment: string, City: string, State: string, Country: string, PostalCode: int, Market: string, Region: string, ProductID: string, Category: string, Sub-Category: string, ProductName: string, Sales: string, Quantity: string, Discount: string, Profit: string, ShippingCost: string, OrderPriority: string]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salesdf.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5c779165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: int, OrderID: string, OrderDate: string, ShipDate: string, ShipMode: string, CustomerID: string, CustomerName: string, Segment: string, City: string, State: string, Country: string, PostalCode: int, Market: string, Region: string, ProductID: string, Category: string, Sub-Category: string, ProductName: string, Sales: string, Quantity: string, Discount: string, Profit: string, ShippingCost: string, OrderPriority: string]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salesdf.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ef11ee75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 74:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "51290"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "salesdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d1ef89a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/25 05:06:42 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: int, OrderID: string, OrderDate: string, ShipDate: string, ShipMode: string, CustomerID: string, CustomerName: string, Segment: string, City: string, State: string, Country: string, PostalCode: int, Market: string, Region: string, ProductID: string, Category: string, Sub-Category: string, ProductName: string, Sales: string, Quantity: string, Discount: string, Profit: string, ShippingCost: string, OrderPriority: string]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.storagelevel import *\n",
    "\n",
    "salesdf.persist(StorageLevel.MEMORY_AND_DISK_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "105c0d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+----------+----------+--------------+----------+----------------+-----------+-------------+---------------+-------------+----------+------+------------+----------------+---------------+------------+--------------------+----------------+--------+--------+----------+------------+-------------+\n",
      "|   ID|        OrderID| OrderDate|  ShipDate|      ShipMode|CustomerID|    CustomerName|    Segment|         City|          State|      Country|PostalCode|Market|      Region|       ProductID|       Category|Sub-Category|         ProductName|           Sales|Quantity|Discount|    Profit|ShippingCost|OrderPriority|\n",
      "+-----+---------------+----------+----------+--------------+----------+----------------+-----------+-------------+---------------+-------------+----------+------+------------+----------------+---------------+------------+--------------------+----------------+--------+--------+----------+------------+-------------+\n",
      "|32298| CA-2012-124891|31/07/2012|31/07/2012|      Same Day|  RH-19495|     Rick Hansen|   Consumer|New York City|       New York|United States|     10024|    US|        East| TEC-AC-10003033|     Technology| Accessories|Plantronics CS510...|         2309.65|       7|       0|  762.1845|     933.57 |     Critical|\n",
      "|26341|  IN-2013-77878|05/02/2013|07/02/2013|  Second Class|  JR-16210|   Justin Ritter|  Corporate|   Wollongong|New South Wales|    Australia|      null|  APAC|     Oceania| FUR-CH-10003950|      Furniture|      Chairs|Novimex Executive...|           Black|3709.395|       9|       0.1|    -288.765|      923.63 |\n",
      "|25330|  IN-2013-71249|17/10/2013|18/10/2013|   First Class|  CR-12730|    Craig Reiter|   Consumer|     Brisbane|     Queensland|    Australia|      null|  APAC|     Oceania| TEC-PH-10004664|     Technology|      Phones|   Nokia Smart Phone|  with Caller ID|5175.171|       9|       0.1|     919.971|      915.49 |\n",
      "|13524|ES-2013-1579342|28/01/2013|30/01/2013|   First Class|  KM-16375|Katherine Murray|Home Office|       Berlin|         Berlin|      Germany|      null|    EU|     Central| TEC-PH-10004583|     Technology|      Phones|Motorola Smart Phone|        Cordless| 2892.51|       5|       0.1|      -96.54|      910.16 |\n",
      "|47221|   SG-2013-4320|05/11/2013|06/11/2013|      Same Day|   RH-9495|     Rick Hansen|   Consumer|        Dakar|          Dakar|      Senegal|      null|Africa|      Africa|TEC-SHA-10000501|     Technology|     Copiers|  Sharp Wireless Fax|      High-Speed| 2832.96|       8|         0|      311.52|      903.04 |\n",
      "|22732|  IN-2013-42360|28/06/2013|01/07/2013|  Second Class|  JM-15655|     Jim Mitchum|  Corporate|       Sydney|New South Wales|    Australia|      null|  APAC|     Oceania| TEC-PH-10000030|     Technology|      Phones| Samsung Smart Phone|  with Caller ID|2862.675|       5|       0.1|     763.275|      897.35 |\n",
      "|30570|  IN-2011-81826|07/11/2011|09/11/2011|   First Class|  TS-21340|   Toby Swindell|   Consumer|      Porirua|     Wellington|  New Zealand|      null|  APAC|     Oceania| FUR-CH-10004050|      Furniture|      Chairs|Novimex Executive...|      Adjustable| 1822.08|       4|         0|      564.84|       894.77|\n",
      "|31192|  IN-2012-86369|14/04/2012|18/04/2012|Standard Class|  MB-18085|      Mick Brown|   Consumer|     Hamilton|        Waikato|  New Zealand|      null|  APAC|     Oceania| FUR-TA-10002958|      Furniture|      Tables|Chromcraft Confer...| Fully Assembled| 5244.84|       6|         0|      996.48|       878.38|\n",
      "|40155| CA-2014-135909|14/10/2014|21/10/2014|Standard Class|  JW-15220|       Jane Waco|  Corporate|   Sacramento|     California|United States|     95823|    US|        West| OFF-BI-10003527|Office Supplies|     Binders|Fellowes PB500 El...|         5083.96|       5|     0.2|  1906.485|     867.69 |          Low|\n",
      "|40936| CA-2012-116638|28/01/2012|31/01/2012|  Second Class|  JH-15985|     Joseph Holt|   Consumer|      Concord| North Carolina|United States|     28027|    US|       South| FUR-TA-10000198|      Furniture|      Tables|Chromcraft Bull-N...|        4297.644|      13|     0.4|-1862.3124|     865.74 |     Critical|\n",
      "|34577| CA-2011-102988|05/04/2011|09/04/2011|  Second Class|  GM-14695|    Greg Maxwell|  Corporate|   Alexandria|       Virginia|United States|     22304|    US|       South| OFF-SU-10002881|Office Supplies|    Supplies|Martin Yale Chadl...|         4164.05|       5|       0|    83.281|     846.54 |         High|\n",
      "|28879|  ID-2012-28402|19/04/2012|22/04/2012|   First Class|  AJ-10780|  Anthony Jacobs|  Corporate|        Kabul|          Kabul|  Afghanistan|      null|  APAC|Central Asia| FUR-TA-10001889|      Furniture|      Tables|Bevis Conference ...| Fully Assembled| 4626.15|       5|         0|      647.55|      835.57 |\n",
      "|45794|   SA-2011-1830|27/12/2011|29/12/2011|  Second Class|   MM-7260| Magdelene Morse|   Consumer|        Jizan|          Jizan| Saudi Arabia|      null|  EMEA|        EMEA|TEC-CIS-10001717|     Technology|      Phones|   Cisco Smart Phone|  with Caller ID| 2616.96|       4|         0|      1151.4|      832.41 |\n",
      "| 4132| MX-2012-130015|13/11/2012|13/11/2012|      Same Day|  VF-21715|  Vicky Freymann|Home Office|       Toledo|         Parana|       Brazil|      null| LATAM|       South| FUR-CH-10002033|      Furniture|      Chairs|Harbour Creations...|      Adjustable|  2221.8|       7|         0|      622.02|      810.25 |\n",
      "|27704|  IN-2013-73951|06/06/2013|08/06/2013|  Second Class|  PF-19120|    Peter Fuller|   Consumer|   Mudanjiang|   Heilongjiang|        China|      null|  APAC|  North Asia| OFF-AP-10003500|Office Supplies|  Appliances|KitchenAid Microwave|           White| 3701.52|      12|         0|     1036.08|      804.54 |\n",
      "|13779|ES-2014-5099955|31/07/2014|03/08/2014|  Second Class|  BP-11185|    Ben Peterman|  Corporate|        Paris|  Ile-de-France|       France|      null|    EU|     Central| OFF-AP-10000423|Office Supplies|  Appliances|Breville Refriger...|             Red|1869.588|       4|       0.1|     186.948|      801.66 |\n",
      "|36178| CA-2014-143567|03/11/2014|06/11/2014|  Second Class|  TB-21175|   Thomas Boland|  Corporate|    Henderson|       Kentucky|United States|     42420|    US|       South| TEC-AC-10004145|     Technology| Accessories|Logitech diNovo E...|         2249.91|       9|       0|  517.4793|     780.70 |     Critical|\n",
      "|12069|ES-2014-1651774|08/09/2014|14/09/2014|Standard Class|  PJ-18835|   Patrick Jones|  Corporate|        Prato|        Tuscany|        Italy|      null|    EU|       South| OFF-AP-10004512|Office Supplies|  Appliances|        Hoover Stove|             Red| 7958.58|      14|         0|     3979.08|      778.32 |\n",
      "|22096|  IN-2014-11763|31/01/2014|01/02/2014|   First Class|  JS-15685|        Jim Sink|  Corporate|   Townsville|     Queensland|    Australia|      null|  APAC|     Oceania| TEC-CO-10000865|     Technology|     Copiers| Brother Fax Machine|      High-Speed|2565.594|       9|       0.1|      28.404|      766.93 |\n",
      "|49463|   TZ-2014-8190|05/12/2014|07/12/2014|  Second Class|   RH-9555| Ritsa Hightower|   Consumer|       Uvinza|         Kigoma|     Tanzania|      null|Africa|      Africa|OFF-KIT-10004058|Office Supplies|  Appliances|    KitchenAid Stove|           White| 3409.74|       6|         0|      818.28|      763.38 |\n",
      "+-----+---------------+----------+----------+--------------+----------+----------------+-----------+-------------+---------------+-------------+----------+------+------------+----------------+---------------+------------+--------------------+----------------+--------+--------+----------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salesdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "eb5d2090",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/25 05:06:42 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: int, OrderID: string, OrderDate: string, ShipDate: string, ShipMode: string, CustomerID: string, CustomerName: string, Segment: string, City: string, State: string, Country: string, PostalCode: int, Market: string, Region: string, ProductID: string, Category: string, Sub-Category: string, ProductName: string, Sales: string, Quantity: string, Discount: string, Profit: string, ShippingCost: string, OrderPriority: string]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salesdf.persist(StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "09e41381",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp = [(1,\"Smith\",1,\"2018\",\"10\",\"M\",3000.00), \n",
    "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000.00), \n",
    "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000.00), \n",
    "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000.00), \n",
    "    (5,\"Brown\",2,\"2010\",\"40\",\"\",300.00), \n",
    "      (6,\"Brown\",2,\"2010\",\"50\",\"\",2000.00) \n",
    "  ]\n",
    "\n",
    "EmpSchema = StructType([  \n",
    "    StructField('Emp_id', IntegerType(), True), \n",
    "    StructField('Empname', StringType(), True),\n",
    "    StructField('MGR', IntegerType(), True), \n",
    "   StructField('YOJ', StringType(), True), \n",
    "     StructField('dept_id', StringType(), True), \n",
    "    StructField('gender', StringType(), True), \n",
    "     StructField('salary', DoubleType(), True) \n",
    "])\n",
    "\n",
    "empDF = spark.createDataFrame(data=emp, schema = EmpSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fb49491a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n",
      "|_c0|_c1| _c2|\n",
      "+---+---+----+\n",
      "|111|zzz|8000|\n",
      "|111|aaa|8888|\n",
      "|121|bbb|8000|\n",
      "+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e73bc64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner join\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 80:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+----+-------+------+------+---------+-------+\n",
      "|Emp_id| Empname|MGR| YOJ|dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---+----+-------+------+------+---------+-------+\n",
      "|     1|   Smith|  1|2018|     10|     M|3000.0|  Finance|     10|\n",
      "|     3|Williams|  1|2010|     10|     M|1000.0|  Finance|     10|\n",
      "|     4|   Jones|  2|2005|     10|     F|2000.0|  Finance|     10|\n",
      "|     2|    Rose|  1|2010|     20|     M|4000.0|Marketing|     20|\n",
      "|     5|   Brown|  2|2010|     40|      | 300.0|       IT|     40|\n",
      "+------+--------+---+----+-------+------+------+---------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dept = [(\"Finance\",10), \n",
    "    (\"Marketing\",20), \n",
    "    (\"Sales\",30), \n",
    "    (\"IT\",40) \n",
    "  ]\n",
    "\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "\n",
    "\n",
    "print(\"Inner join\")\n",
    "empDF.join(deptDF,empDF.dept_id ==  deptDF.dept_id,\"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1a23ed29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 85:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+----+-------+------+------+---------+-------+\n",
      "|Emp_id| Empname| MGR| YOJ|dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+----+----+-------+------+------+---------+-------+\n",
      "|     1|   Smith|   1|2018|     10|     M|3000.0|  Finance|     10|\n",
      "|     3|Williams|   1|2010|     10|     M|1000.0|  Finance|     10|\n",
      "|     4|   Jones|   2|2005|     10|     F|2000.0|  Finance|     10|\n",
      "|     2|    Rose|   1|2010|     20|     M|4000.0|Marketing|     20|\n",
      "|  null|    null|null|null|   null|  null|  null|    Sales|     30|\n",
      "|     5|   Brown|   2|2010|     40|      | 300.0|       IT|     40|\n",
      "|     6|   Brown|   2|2010|     50|      |2000.0|     null|   null|\n",
      "+------+--------+----+----+-------+------+------+---------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.dept_id ==  deptDF.dept_id,\"full\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6946f8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+----+-------+------+------+---------+-------+\n",
      "|Emp_id| Empname|MGR| YOJ|dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---+----+-------+------+------+---------+-------+\n",
      "|     1|   Smith|  1|2018|     10|     M|3000.0|  Finance|     10|\n",
      "|     3|Williams|  1|2010|     10|     M|1000.0|  Finance|     10|\n",
      "|     2|    Rose|  1|2010|     20|     M|4000.0|Marketing|     20|\n",
      "|     6|   Brown|  2|2010|     50|      |2000.0|     null|   null|\n",
      "|     4|   Jones|  2|2005|     10|     F|2000.0|  Finance|     10|\n",
      "|     5|   Brown|  2|2010|     40|      | 300.0|       IT|     40|\n",
      "+------+--------+---+----+-------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "empDF.join(deptDF,empDF.dept_id ==  deptDF.dept_id,\"left\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "57e23378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+----+-------+------+------+---------+-------+\n",
      "|Emp_id| Empname| MGR| YOJ|dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+----+----+-------+------+------+---------+-------+\n",
      "|     4|   Jones|   2|2005|     10|     F|2000.0|  Finance|     10|\n",
      "|     3|Williams|   1|2010|     10|     M|1000.0|  Finance|     10|\n",
      "|     1|   Smith|   1|2018|     10|     M|3000.0|  Finance|     10|\n",
      "|     2|    Rose|   1|2010|     20|     M|4000.0|Marketing|     20|\n",
      "|  null|    null|null|null|   null|  null|  null|    Sales|     30|\n",
      "|     5|   Brown|   2|2010|     40|      | 300.0|       IT|     40|\n",
      "+------+--------+----+----+-------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.dept_id ==  deptDF.dept_id,\"right\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fb78c520",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+----+-------+------+------+\n",
      "|Emp_id|Empname|MGR| YOJ|dept_id|gender|salary|\n",
      "+------+-------+---+----+-------+------+------+\n",
      "|     6|  Brown|  2|2010|     50|      |2000.0|\n",
      "+------+-------+---+----+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "empDF.join(deptDF,empDF.dept_id ==  deptDF.dept_id,\"leftanti\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a09d1741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 108:>                                                        (0 + 2) / 2]\r",
      "\r",
      "[Stage 108:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+----+-------+------+------+\n",
      "|Emp_id| Empname|MGR| YOJ|dept_id|gender|salary|\n",
      "+------+--------+---+----+-------+------+------+\n",
      "|     1|   Smith|  1|2018|     10|     M|3000.0|\n",
      "|     3|Williams|  1|2010|     10|     M|1000.0|\n",
      "|     4|   Jones|  2|2005|     10|     F|2000.0|\n",
      "|     2|    Rose|  1|2010|     20|     M|4000.0|\n",
      "|     5|   Brown|  2|2010|     40|      | 300.0|\n",
      "+------+--------+---+----+-------+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "empDF.join(deptDF,empDF.dept_id ==  deptDF.dept_id,\"leftsemi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0d4a77fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "empDF.createOrReplaceTempView(\"emp\")\n",
    "empDF.createOrReplaceTempView(\"dept\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7c0f9f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 113:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+\n",
      "|Emp_id| Empname|dept_id|\n",
      "+------+--------+-------+\n",
      "|     1|   Smith|     10|\n",
      "|     1|   Smith|     10|\n",
      "|     1|   Smith|     10|\n",
      "|     3|Williams|     10|\n",
      "|     3|Williams|     10|\n",
      "|     3|Williams|     10|\n",
      "|     4|   Jones|     10|\n",
      "|     4|   Jones|     10|\n",
      "|     4|   Jones|     10|\n",
      "|     2|    Rose|     20|\n",
      "|     5|   Brown|     40|\n",
      "|     6|   Brown|     50|\n",
      "+------+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select emp.Emp_id, emp.Empname, dept.dept_id from emp inner join dept on emp.dept_id = dept.dept_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1c66435d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 118:>                                                        (0 + 2) / 2]\r",
      "\r",
      "[Stage 118:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+\n",
      "|Emp_id| Empname|dept_id|\n",
      "+------+--------+-------+\n",
      "|     2|    Rose|     20|\n",
      "|     1|   Smith|     10|\n",
      "|     1|   Smith|     10|\n",
      "|     1|   Smith|     10|\n",
      "|     3|Williams|     10|\n",
      "|     3|Williams|     10|\n",
      "|     3|Williams|     10|\n",
      "|     5|   Brown|     40|\n",
      "|     4|   Jones|     10|\n",
      "|     4|   Jones|     10|\n",
      "|     4|   Jones|     10|\n",
      "|     6|   Brown|     50|\n",
      "+------+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select emp.Emp_id, emp.Empname, dept.dept_id from emp left join dept on emp.dept_id = dept.dept_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0809bae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+\n",
      "|Emp_id| Empname|dept_id|\n",
      "+------+--------+-------+\n",
      "|     2|    Rose|     20|\n",
      "|     4|   Jones|     10|\n",
      "|     3|Williams|     10|\n",
      "|     1|   Smith|     10|\n",
      "|     4|   Jones|     10|\n",
      "|     3|Williams|     10|\n",
      "|     1|   Smith|     10|\n",
      "|     5|   Brown|     40|\n",
      "|     4|   Jones|     10|\n",
      "|     3|Williams|     10|\n",
      "|     1|   Smith|     10|\n",
      "|     6|   Brown|     50|\n",
      "+------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select emp.Emp_id, emp.Empname, dept.dept_id from emp right join dept on emp.dept_id = dept.dept_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5d93e4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 130:>                                                        (0 + 2) / 2]\r",
      "\r",
      "[Stage 130:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+\n",
      "|Emp_id| Empname|dept_id|\n",
      "+------+--------+-------+\n",
      "|     1|   Smith|     10|\n",
      "|     1|   Smith|     10|\n",
      "|     1|   Smith|     10|\n",
      "|     3|Williams|     10|\n",
      "|     3|Williams|     10|\n",
      "|     3|Williams|     10|\n",
      "|     4|   Jones|     10|\n",
      "|     4|   Jones|     10|\n",
      "|     4|   Jones|     10|\n",
      "|     2|    Rose|     20|\n",
      "|     5|   Brown|     40|\n",
      "|     6|   Brown|     50|\n",
      "+------+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select emp.Emp_id, emp.Empname, dept.dept_id from emp full outer join dept on emp.dept_id = dept.dept_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "261746bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+----+-------+------+------+\n",
      "|Emp_id| Empname|MGR| YOJ|dept_id|gender|salary|\n",
      "+------+--------+---+----+-------+------+------+\n",
      "|     5|   Brown|  2|2010|     40|      | 300.0|\n",
      "|     3|Williams|  1|2010|     10|     M|1000.0|\n",
      "|     4|   Jones|  2|2005|     10|     F|2000.0|\n",
      "|     6|   Brown|  2|2010|     50|      |2000.0|\n",
      "|     1|   Smith|  1|2018|     10|     M|3000.0|\n",
      "|     2|    Rose|  1|2010|     20|     M|4000.0|\n",
      "+------+--------+---+----+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.orderBy(['salary'], ascending = [True]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b619ce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = empDF.join(deptDF,empDF.dept_id ==  deptDF.dept_id,\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7f1cd700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 136:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+----+-------+------+------+---------+-------+\n",
      "|Emp_id| Empname| MGR| YOJ|dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+----+----+-------+------+------+---------+-------+\n",
      "|     1|   Smith|   1|2018|     10|     M|3000.0|  Finance|     10|\n",
      "|     3|Williams|   1|2010|     10|     M|1000.0|  Finance|     10|\n",
      "|     4|   Jones|   2|2005|     10|     F|2000.0|  Finance|     10|\n",
      "|     2|    Rose|   1|2010|     20|     M|4000.0|Marketing|     20|\n",
      "|  null|    null|null|null|   null|  null|  null|    Sales|     30|\n",
      "|     5|   Brown|   2|2010|     40|      | 300.0|       IT|     40|\n",
      "|     6|   Brown|   2|2010|     50|      |2000.0|     null|   null|\n",
      "+------+--------+----+----+-------+------+------+---------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbe655b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8238a75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0459693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c10c1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
